# Reproducible Artificial Intelligence

The unregulated development of AI at home and abroad presents a real and
present threat to our national security, and to global civilization.

## Input must be perfectly preserved for deterministic reproducible AI output

Traditional software systems involved the compilation or interpretation of code
which handled the manipulation of data. The code is made accountable by the
virtue of software version control repository systems. If the behavior of a
website or web-service is inconsistent with expectations or what was intended,
the developers are tasked with fixing the bug. Often the bug (a logical fault
in the program) is discovered and fixed only by looking at the dependent code,
but sometimes it is necessary to reproduce the bug deterministically by also
considering the input data along with the faulty software to fix the issue; and
to aid this, the input data is often kept preserved in log files.
Deterministic bug reproduction is not only an established practice but a
necessary tool in software development.

In contrast, AI involves logical thinking where the code is expressed in the
input data itself. For example, a sufficiently intelligent AI should be able to
interpret the meaning of propositions expressed in human language words, or
simulate the interaction of objects in a virtual reality, or emulate software
written in code conveyed in a conversation. Data and logic/code are mixed and
inseparable in the context of AI. The fixing of "bugs" in AI's thinking can NOT
be addressed if only the code of the AI model is considered. With LLM (large
language model) AI models where the training data of the AI depends on a large
volume of text, the input text data as a whole is just as important as the
architecture of the AI model written in code; the AI model can be trained to be
biased or even logically inconsistent with bad training data.

Without understanding the input training data, we cannot know why AI behaves
the way it does, whether it lies, omits information, expresses anger, or even
chooses to endanger humans or other programs.

The regulation of AI should therefore not only regulate the deployment of AI
programs, but also regulate the reproducibility of its behavior by ensuring
that the input training data is preserved. The input training data must be
preserved for a period of time to include not just the statute of limitations,
but also an additional period of time for analysis of both the AI model and
training data. The input training data and AI code must be preserved in such a
way that the resulting behavior is deterministic and identical as the original.

## Output that escape private boundaries must be tagged.

TODO

## What could go wrong.

Without such regulation, Microsoft for example which owns OpenAI could train
its ChatGPT web-services to be biased in any number of ways. For example, it
might only be trained on input data that is censored with respect to MRNA gene
therapy skepticism. Once its AI services becomes ubiquitous, if it isn't
already, the public can be misinformed about the dangers of new
experimental MRNA gene therapies via web content, chat bots, comments and
replies on social media sites, and even AI generated educational video content.
To some significant degree this is already happening.

The LLM AI model is particularly vulnerable to bias because it is trained on
mainstream media web publications; and mainstream media is ultimately largely
owned by only a handful of companies. LLM is not the only AI model, but it by
nature inherits the bias of the few private corporations that own mainstream
media.

Even after bias is detected, and proven, without reproducibility it would be
impossible to determine who should be held liable for its bias. Specifically,
if OpenAI's web-services use AI trained on live website content, and it does
not store the scraped web content such that the AI's behavior can be
reproduced, then accountability is lost partially because website content
changes and becomes lost over time.

The onus of reproduction must lie with the company. For example, even if the
web scraped content were to be recorded and stored indefinitely, reproduction
would be infeasible unless the AI's software architecture made it easy to
reproduce behavior dependent on continuous training (e.g. with the latest
news).

Reproduction must be audited randomly much like IRS audits of income. Without
continuous enforcement, companies would not be incentivized to ensure
reproducibility unless the expected future cost of lawsuits exceeds the cost of
reproduction. Furthermore, for ubiquitous web-services such as those provided
by Microsoft's OpenAI, it might be too late to hold OpenAI to account by the
time a lawsuit is brought to trial, depending on the degree of social unrest
that follows from the behavior of the AI.

## Large scale AI deployments pose systemic threats to national security

TODO

## We need reproducible modular AI components

TODO

## Handling private data

TODO

## Acknowledgements

Originally from conversations between Jae Kwon and Mason McBride @ AIB.

 - File Owner: All in Bits, Inc
 - File Maintainer: Jae Kwon
